---
title: "Chapter 3 - Data Transformation with dplyr"
#date: 2018-01-18
output: 
  html_document: 
    toc: true
    toc_float: true
    #number_sections: true
    #code_folding: show
---

```{r options, include=FALSE}
set.seed(1014)
#options(digits = 3)

knitr::opts_chunk$set(comment = "#>",highlight=FALSE,collapse = TRUE,cache = TRUE,out.width = "70%",fig.align = 'center',fig.width = 8,fig.asp = 0.618)

options(dplyr.print_min = 6, dplyr.print_max = 6)
library(htmltools)
```

## Introduction

Visualisation is an important tool for insight generation, but it is rare that you get the data in exactly the right form you need. Often you'll need to create some new variables or summaries, or maybe you just want to rename the variables or reorder the observations in order to make the data a little easier to work with. You'll learn how to do all that (and more!) in this chapter, which will teach you how to transform your data using the dplyr package and a new dataset on flights departing New York City in 2013.

### Prerequisites

In this chapter we're going to focus on how to use the dplyr package, another core member of the tidyverse. We'll illustrate the key ideas using data from the nycflights13 package, and use ggplot2 to help us understand the data. 

#### Libraries ***

```{r setup, message = TRUE}
library(nycflights13)
library(tidyverse)
library(dplyr)
library(ggplot2)
```

Take careful note of the conflicts message that's printed when you load the tidyverse. It tells you that dplyr overwrites some functions in base R. If you want to use the base version of these functions after loading dplyr, you'll need to use their full names: `stats::filter()` and `stats::lag()`.

### nycflights13

To explore the basic data manipulation verbs of dplyr, we'll use `nycflights13::flights`. This data frame contains all `r format(nrow(nycflights13::flights), big.mark = ",")` flights that departed from New York City in 2013. The data comes from the US [Bureau of Transportation Statistics](http://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120&Link=0), and is documented in `?flights`.

```{r}
flights
```

You might notice that this data frame prints a little differently from other data frames you might have used in the past: it only shows the first few rows and all the columns that fit on one screen. (To see the whole dataset, you can run `View(flights)` which will open the dataset in the RStudio viewer). It prints differently because it's a __tibble__. Tibbles are data frames, but slightly tweaked to work better in the tidyverse. For now, you don't need to worry about the differences; we'll come back to tibbles in more detail in [wrangle](#wrangle-intro).
 
You might also have noticed the row of three (or four) letter abbreviations under the column names. These describe the type of each variable:

* `int` stands for integers.

* `dbl` stands for doubles, or real numbers.

* `chr` stands for character vectors, or strings.

* `dttm` stands for date-times (a date + a time).

There are three other common types of variables that aren't used in this dataset but you'll encounter later in the book:

* `lgl` stands for logical, vectors that contain only `TRUE` or `FALSE`.

* `fctr` stands for factors, which R uses to represent categorical variables
  with fixed possible values.

* `date` stands for dates.

### dplyr basics

In this chapter you are going to learn the five key dplyr functions that allow you to solve the vast majority of your data manipulation challenges:

* Pick observations by their values (`filter()`).
* Reorder the rows (`arrange()`).
* Pick variables by their names (`select()`).
* Create new variables with functions of existing variables (`mutate()`).
* Collapse many values down to a single summary (`summarise()`).

These can all be used in conjunction with `group_by()` which changes the scope of each function from operating on the entire dataset to operating on it group-by-group. These six functions provide the verbs for a language of data manipulation.

All verbs work similarly: 

1.  The first argument is a data frame.

1.  The subsequent arguments describe what to do with the data frame,
    using the variable names (without quotes).
    
1.  The result is a new data frame.

Together these properties make it easy to chain together multiple simple steps to achieve a complex result. Let's dive in and see how these verbs work.

## Filter rows with `filter()` {#filter}

`filter()` allows you to subset observations based on their values. The first argument is the name of the data frame. The second and subsequent arguments are the expressions that filter the data frame. For example, we can select all flights on January 1st with:

```{r}
filter(flights, month == 1, day == 1)
```

When you run that line of code, dplyr executes the filtering operation and returns a new data frame. dplyr functions never modify their inputs, so if you want to save the result, you'll need to use the assignment operator, `<-`:

```{r}
jan1 <- filter(flights, month == 1, day == 1)
```

R either prints out the results, or saves them to a variable. If you want to do both, you can wrap the assignment in parentheses:

```{r}
(dec25 <- filter(flights, month == 12, day == 25))
```

### Comparisons

To use filtering effectively, you have to know how to select the observations that you want using the comparison operators. R provides the standard suite: `>`, `>=`, `<`, `<=`, `!=` (not equal), and `==` (equal). 

When you're starting out with R, the easiest mistake to make is to use `=` instead of `==` when testing for equality. When this happens you'll get an informative error:

```{r, error = TRUE}
filter(flights, month = 1)
```

There's another common problem you might encounter when using `==`: floating point numbers. These results might surprise you!

```{r}
sqrt(2) ^ 2 == 2
1/49 * 49 == 1
```

##### near(), sqrt()

Computers use finite precision arithmetic (they obviously can't store an infinite number of digits!) so remember that every number you see is an approximation. Instead of relying on `==`, use `near()`:

```{r}
near(sqrt(2) ^ 2,  2)
near(1 / 49 * 49, 1)
```

### Logical operators

Multiple arguments to `filter()` are combined with "and": every expression must be true in order for a row to be included in the output. For other types of combinations, you'll need to use Boolean operators yourself: `&` is "and", `|` is "or", and `!` is "not". Figure \@ref(fig:bool-ops) shows the complete set of Boolean operations.

```{r bool-ops, echo = FALSE, fig.cap = "Complete set of boolean operations. `x` is the left-hand circle, `y` is the right-hand circle, and the shaded region show which parts each operator selects."}
knitr::include_graphics("diagrams/transform-logical.png")
```

The following code finds all flights that departed in November or December:

```{r, eval = FALSE}
filter(flights, month == 11 | month == 12)
```

The order of operations doesn't work like English. You can't write `filter(flights, month == 11 | 12)`, which you might literally translate into  "finds all flights that departed in November or December". Instead it finds all months that equal `11 | 12`, an expression that evaluates to `TRUE`. In a numeric context (like here), `TRUE` becomes one, so this finds all flights in January, not November or December. This is quite confusing!

A useful short-hand for this problem is `x %in% y`. This will select every row where `x` is one of the values in `y`. We could use it to rewrite the code above:

```{r, eval = FALSE}
nov_dec <- filter(flights, month %in% c(11, 12))
```

Sometimes you can simplify complicated subsetting by remembering De Morgan's law: `!(x & y)` is the same as `!x | !y`, and `!(x | y)` is the same as `!x & !y`. For example, if you wanted to find flights that weren't delayed (on arrival or departure) by more than two hours, you could use either of the following two filters:

```{r, eval = FALSE}
filter(flights, !(arr_delay > 120 | dep_delay > 120))
filter(flights, arr_delay <= 120, dep_delay <= 120)
```

As well as `&` and `|`, R also has `&&` and `||`. Don't use them here! You'll learn when you should use them in [conditional execution].

Whenever you start using complicated, multipart expressions in `filter()`, consider making them explicit variables instead. That makes it much easier to check your work. You'll learn how to create new variables shortly.

### Missing values

One important feature of R that can make comparison tricky are missing values, or `NA`s ("not availables"). `NA` represents an unknown value so missing values are "contagious": almost any operation involving an unknown value will also be unknown.

```{r}
NA > 5
10 == NA
NA + 10
NA / 2
```

The most confusing result is this one:

```{r}
NA == NA
```

It's easiest to understand why this is true with a bit more context:

```{r}
# Let x be Mary's age. We don't know how old she is.
x <- NA

# Let y be John's age. We don't know how old he is.
y <- NA

# Are John and Mary the same age?
x == y
# We don't know!
```

##### is.na()

If you want to determine if a value is missing, use `is.na()`:

```{r}
is.na(x)
```

`filter()` only includes rows where the condition is `TRUE`; it excludes both `FALSE` and `NA` values. If you want to preserve missing values, ask for them explicitly:

```{r}
df <- tibble(x = c(1, NA, 3))
filter(df, x > 1)
filter(df, is.na(x) | x > 1)
```

### Exercises

1.  Find all flights that

    1. Had an arrival delay of two or more hours
    
        ```{r eval=FALSE}
        filter(flights, arr_delay > 120)
        ```
        
    1. Flew to Houston (`IAH` or `HOU`)
    
        ```{r eval=FALSE}
        filter(flights, dest == "IAH" | dest == "HOU")
        ```
    
    1. Were operated by United, American, or Delta
    
        ```{r eval=FALSE}
        filter(flights, carrier =="AA" | carrier == "UA" | carrier =="DL")
        
        # OR
        
        filter(flights, carrier %in% c("AA", "UA", "DL"))
        ```
        
    1. Departed in summer (July, August, and September)
    
        ```{r eval=FALSE}
        filter(flights, month %in% c(7, 8, 9))
        ```
    
    1. Arrived more than two hours late, but didn't leave late
    
        ```{r eval=FALSE}
        filter(flights, arr_delay > 120 & dep_delay <= 0)
        ```
    
    1. Were delayed by at least an hour, but made up over 30 minutes in flight
    
        ```{r eval=FALSE}
        filter(flights, dep_delay > 60 & (arr_time < (sched_arr_time + 30)))
        ```

    1. Departed between midnight and 6am (inclusive)
    
        ```{r eval=FALSE}
        filter(flights, dep_time >= 0000 & dep_time <= 0600)
        ```

##### between()

1.  Another useful dplyr filtering helper is `between()`. What does it do?
    Can you use it to simplify the code needed to answer the previous 
    challenges?
    
    ```{r eval=FALSE}
    filter(flights, between(dep_time, 0000, 0600))
    ```

1.  How many flights have a missing `dep_time`? What other variables are 
    missing? What might these rows represent?

    ```{r eval=FALSE}
    filter(flights,is.na(dep_time)) # 8255 flights
    
    # What other variables are missing? dep_delay, arr_time, arr_delay
    
    # What might these rows represent? Cancelled flights
    ```
    
1.  Why is `NA ^ 0` not missing? Why is `NA | TRUE` not missing?
    Why is `FALSE & NA` not missing? Can you figure out the general
    rule?  (`NA * 0` is a tricky counterexample!)
    
    ```{r eval=FALSE}
    NA ^ 0
    # [1] 1
    
    NA | TRUE
    #  [1] TRUE
    
    NA & FALSE
    # [1] FALSE
    
    NA * 0
    # [1] NA
    ```
    

## Arrange rows with `arrange()`

`arrange()` works similarly to `filter()` except that instead of selecting rows, it changes their order. It takes a data frame and a set of column names (or more complicated expressions) to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns:

```{r}
arrange(flights, year, month, day)
```

Use `desc()` to re-order by a column in descending order:

```{r}
arrange(flights, desc(arr_delay))
```

Missing values are always sorted at the end:

```{r}
df <- tibble(x = c(5, 2, NA))
arrange(df, x)
arrange(df, desc(x))
```

### Exercises

1.  How could you use `arrange()` to sort all missing values to the start?
    (Hint: use `is.na()`).
    
    ```{r eval=FALSE}
    arrange(df, desc(is.na(x)))
    
    ```

1.  Sort `flights` to find the most delayed flights. Find the flights that
    left earliest.
      
    ```{r eval=FALSE}
    arrange(flights, desc(dep_delay))
    arrange(flights, dep_delay)
    
    ```

1.  Sort `flights` to find the fastest flights.

    ```{r eval=FALSE}
    arrange(flights, air_time)
    select(arrange(flights, air_time), flight, month, day, year, air_time)
    
    ```

1.  Which flights traveled the longest? Which traveled the shortest?

    ```{r eval=FALSE}
    select(arrange(flights, desc(air_time)), flight, month, day, year, air_time) 
    select(arrange(flights, air_time), flight, month, day, year, air_time)
      
    ```

## Select columns with `select()` {#select}

It's not uncommon to get datasets with hundreds or even thousands of variables. In this case, the first challenge is often narrowing in on the variables you're actually interested in. `select()` allows you to rapidly zoom in on a useful subset using operations based on the names of the variables.

`select()` is not terribly useful with the flights data because we only have 19 variables, but you can still get the general idea:

```{r}
# Select columns by name
select(flights, year, month, day)
# Select all columns between year and day (inclusive)
select(flights, year:day)
# Select all columns except those from year to day (inclusive)
select(flights, -(year:day))
```

#### starts_with(), ends_with(), contains(), matches(), num_range(), one_of()

There are a number of helper functions you can use within `select()`:

* `starts_with("abc")`: matches names that begin with "abc".

* `ends_with("xyz")`: matches names that end with "xyz".

* `contains("ijk")`: matches names that contain "ijk".

* `matches("(.)\\1")`: selects variables that match a regular expression.
   This one matches any variables that contain repeated characters. You'll 
   learn more about regular expressions in [strings].
   
*  `num_range("x", 1:3)` matches `x1`, `x2` and `x3`.
   
See `?select` for more details.

![](images/select-helper-functions.png)

##### rename()

`select()` can be used to rename variables, but it's rarely useful because it drops all of the variables not explicitly mentioned. Instead, use __`rename()`__, which is a variant of `select()` that keeps all the variables that aren't explicitly mentioned:

```{r}
rename(flights, tail_num = tailnum) # new = old
```

##### everything()

Another option is to use `select()` in conjunction with the `everything()` helper. This is useful if you have a handful of variables you'd like to __move to the start of the data frame__.

```{r}
select(flights, time_hour, air_time, everything())
```

### Exercises


1.  Brainstorm as many ways as possible to select `dep_time`, `dep_delay`,
    `arr_time`, and `arr_delay` from `flights`.
    ```{r eval=FALSE}
    select(flights, dep_time, dep_delay, arr_time, arr_delay) # YES x4
    select(flights, starts_with("dep"), starts_with("arr")) # YES x4
    select(flights, ends_with("_time"), ends_with("_delay")) #NO x7
    select(flights, contains("arr"), contains("dep")) #NO x7
    ```
   
1.  What happens if you include the name of a variable multiple times in
    a `select()` call?
    
    It is returned once, not mulitple times
  
1.  What does the `one_of()` function do? Why might it be helpful in conjunction
    with this vector?
    
    ```{r}
    vars <- c("year", "month", "day", "dep_delay", "arr_delay")
    ```
    
    ```{r}
    vars
    ```
    
    These functions allow you to select variables based on their names:
    - starts_with(): starts with a prefix
    - ends_with(): ends with a prefix
    - contains(): contains a literal string
    - matches(): matches a regular expression
    - num_range(): a numerical range like x01, x02, x03.
    - one_of(): variables in character vector.
    - everything(): all variables.
    
    ```{r eval=TRUE}
    select(flights, one_of(vars)) #returns each column in vars

    ```
    
    
1.  Does the result of running the following code surprise you?  How do the
    select helpers deal with case by default? How can you change that default?

    ```{r, eval = FALSE}
    select(flights, contains("TIME"))
    ```

    Yes, it is a surprise that helpers are not case sensitive.
    How do the select helpers deal with case by default? 
    - ignore.case	If TRUE, the default, ignores case when matching names.
    How can you change that default?

    ```{r eval=FALSE}
    select(flights, contains("TIME", ignore.case = FALSE))
    
    ```

## Add new variables with `mutate()` {#mutate}

Besides selecting sets of existing columns, it's often useful to add new columns that are functions of existing columns. That's the job of `mutate()`. 

`mutate()` always adds new columns at the end of your dataset so we'll start by creating a narrower dataset so we can see the new variables. Remember that when you're in RStudio, the easiest way to see all the columns is `View()`.


##### flight_sml ***
```{r}
flights_sml <- select(flights, 
  year:day, 
  ends_with("delay"), 
  distance, 
  air_time
)
```

```{r}
mutate(flights_sml,
  gain = arr_delay - dep_delay,
  speed = distance / air_time * 60
)
```

Note that you can refer to columns that you've just created:

```{r}
mutate(flights_sml,
  gain = arr_delay - dep_delay,
  hours = air_time / 60,
  gain_per_hour = gain / hours
)
```

##### transmute()

If you only want to keep the new variables, use `transmute()`:

```{r}
transmute(flights,
  gain = arr_delay - dep_delay,
  hours = air_time / 60,
  gain_per_hour = gain / hours
)
```

### Useful creation functions {#mutate-funs}

There are many functions for creating new variables that you can use with `mutate()`. The key property is that the function must be vectorised: it must take a vector of values as input, return a vector with the same number of values as output. There's no way to list every possible function that you might use, but here's a selection of functions that are frequently useful:

##### Arithmetic operators

*   Arithmetic operators: `+`, `-`, `*`, `/`, `^`. These are all vectorised,
    using the so called "recycling rules". If one parameter is shorter than 
    the other, it will be automatically extended to be the same length. This 
    is most useful when one of the arguments is a single number: `air_time / 60`,
    `hours * 60 + minute`, etc.
    
    Arithmetic operators are also useful in conjunction with the aggregate
    functions you'll learn about later. For example, `x / sum(x)` calculates 
    the proportion of a total, and `y - mean(y)` computes the difference from 
    the mean.
    
##### Modular arithmetic

*   Modular arithmetic: `%/%` (integer division) and `%%` (remainder), where
    `x == y * (x %/% y) + (x %% y)`. Modular arithmetic is a handy tool because 
    it allows you to break integers up into pieces. For example, in the 
    flights dataset, you can compute `hour` and `minute` from `dep_time` with:
    
    ```{r}
    transmute(flights,
      dep_time,
      hour = dep_time %/% 100,
      minute = dep_time %% 100
    )
    ```
    
##### Logs: `log()`, `log2()`, `log10()`

*   Logs: `log()`, `log2()`, `log10()`. Logarithms are an incredibly useful
    transformation for dealing with data that ranges across multiple orders of
    magnitude. They also convert multiplicative relationships to additive, a
    feature we'll come back to in modelling.
    
    All else being equal, I recommend using `log2()` because it's easy to
    interpret: a difference of 1 on the log scale corresponds to doubling on
    the original scale and a difference of -1 corresponds to halving.

##### Offsets: `lead()` and `lag()`

*   Offsets: `lead()` and `lag()` allow you to refer to leading or lagging 
    values. This allows you to compute running differences (e.g. `x - lag(x)`) 
    or find when values change (`x != lag(x))`. They are most useful in 
    conjunction with `group_by()`, which you'll learn about shortly.
    
    ```{r}
    (x <- 1:10)
    lag(x)
    lead(x)
    ```

##### Cumulative and rolling aggregates: `cumsum()`, `cumprod()`, `cummin()`, `cummax()`, `cummean()`

*   Cumulative and rolling aggregates: R provides functions for running sums,
    products, mins and maxes: `cumsum()`, `cumprod()`, `cummin()`, `cummax()`; 
    and dplyr provides `cummean()` for cumulative means. If you need rolling
    aggregates (i.e. a sum computed over a rolling window), try the RcppRoll
    package.
    
    ```{r}
    x
    cumsum(x)
    cummean(x)
    ```

*   Logical comparisons, `<`, `<=`, `>`, `>=`, `!=`, which you learned about
    earlier. If you're doing a complex sequence of logical operations it's 
    often a good idea to store the interim values in new variables so you can
    check that each step is working as expected.

*   Ranking: there are a number of ranking functions, but you should 
    start with `min_rank()`. It does the most usual type of ranking 
    (e.g. 1st, 2nd, 2nd, 4th). The default gives smallest values the small
    ranks; use `desc(x)` to give the largest values the smallest ranks. 
    
    ```{r}
    y <- c(1, 2, 2, NA, 3, 4)
    min_rank(y)
    min_rank(desc(y))
    ```
    
    If `min_rank()` doesn't do what you need, look at the variants
    `row_number()`, `dense_rank()`, `percent_rank()`, `cume_dist()`,
    `ntile()`.  See their help pages for more details.
    
    ```{r}
    row_number(y)
    dense_rank(y)
    percent_rank(y)
    cume_dist(y)
    ```

### Exercises

```{r, eval = FALSE, echo = FALSE}
flights <- flights %>% mutate(
  dep_time = hour * 60 + minute,
  arr_time = (arr_time %/% 100) * 60 + (arr_time %% 100),
  airtime2 = arr_time - dep_time,
  dep_sched = dep_time + dep_delay
)

ggplot(flights, aes(dep_sched)) + geom_histogram(binwidth = 60)
ggplot(flights, aes(dep_sched %% 60)) + geom_histogram(binwidth = 1)
ggplot(flights, aes(air_time - airtime2)) + geom_histogram()
```

1.  Currently `dep_time` and `sched_dep_time` are convenient to look at, but
    hard to compute with because they're not really continuous numbers. 
    Convert them to a more convenient representation of number of minutes
    since midnight.
    
    ```{r eval=FALSE}
    transmute(flights,
              dep_time,
              dep_hour = dep_time %/% 100,
              dep_min = dep_time %% 100,
              dep_mins_since_midnight = (dep_hour * 60) + dep_min,
              sched_dep_time,
              sched_dep_mins_since_midnight = ((sched_dep_time %/% 100) * 60) + sched_dep_time %% 100
              )
    ```
    
    
1.  Compare `air_time` with `arr_time - dep_time`. What do you expect to see?
    What do you see? What do you need to do to fix it?
    
    ```{r eval=TRUE}
    transmute(flights,
          air_time,
          enroute = arr_time - dep_time
          )
    ```
    
    To fix it, the arr_time and dep_time must be converted to minutes from midnight.      However, they are also given in local time zones, so that must be accounted for.      As well, some flights could have departed before midnight and landed after            midnight, which must also be accounted for. 

    Naive results show air_time most often varies from arr_time - dep_time:
    
    ```{r eval=TRUE}
    transmute(flights,
      dep_time = ((dep_time %/% 100) * 60) + (dep_time %% 100),
      arr_time = ((arr_time %/% 100) * 60) + (arr_time %% 100),
      enroute = arr_time - dep_time %% (60*24),
      air_time
      )
    ```


1.  Compare `dep_time`, `sched_dep_time`, and `dep_delay`. How would you
    expect those three numbers to be related?
    
    ```{r eval=FALSE}
    
    # One might expect dep_time - sched_dep_time == dep_delay
    
    transmute(flights, dep_time, sched_dep_time, dep_delay, x = dep_time - sched_dep_time, y = x == dep_delay)
    
    # However, since arr_time and dep_time are in clock times rather then minutes, an adjustment would need be made (as above) to effect the correct calculation.
    
    ```
    

1.  Find the 10 most delayed flights using a ranking function. How do you want 
    to handle ties? Carefully read the documentation for `min_rank()`.
    
    ```{r eval=FALSE}
    ?min_rank
    # equivalent to rank(ties.method = "min")
    
    ```
    
    ```{r}
    x <- as.data.frame(select(flights, dep_delay))
    head(min_rank(desc(x)), 10)   
    
    ```


1.  What does `1:3 + 1:10` return? Why?

    ```{r, warning=TRUE}
    1:3 + 1:10
    # returns a warning message because longer object length is not a multiple of shorter object length
    ```

1.  What trigonometric functions does R provide?

    Base R provides the following, with angles in radians `cos(x)`, `sin(x)`, `tan(x)`, `acos(x)`, `asin(x)`, `atan(x)`, `atan2(y, x)`, `cospi(x)`, `sinpi(x)`, `tanpi(x)`

## Grouped summaries with `summarise()`

The last key verb is `summarise()`. It collapses a data frame to a single row:

```{r}
summarise(flights, delay = mean(dep_delay, na.rm = TRUE))
```

(We'll come back to what that `na.rm = TRUE` means very shortly.)

### group_by()

`summarise()` is not terribly useful unless we pair it with `group_by()`. This changes the unit of analysis from the complete dataset to individual groups. Then, when you use the dplyr verbs on a grouped data frame they'll be automatically applied "by group". For example, if we applied exactly the same code to a data frame grouped by date, we get the average delay per date:

```{r}
by_day <- group_by(flights, year, month, day)
summarise(by_day, delay = mean(dep_delay, na.rm = TRUE))
```

_Note: Grouping doesn't change how the data looks (apart from listing how it's grouped). It changes how it acts with the other dplyr verbs._

Together `group_by()` and `summarise()` provide one of the tools that you'll use most commonly when working with dplyr: grouped summaries. But before we go any further with this, we need to introduce a powerful new idea: the pipe.

### Combine multiple operations with the Pipe

Imagine that we want to explore the relationship between the distance and average delay for each location. Using what you know about dplyr, you might write code like this:

```{r, fig.width = 6}
by_dest <- group_by(flights, dest)
delay <- summarise(by_dest,
  count = n(),
  dist = mean(distance, na.rm = TRUE),
  delay = mean(arr_delay, na.rm = TRUE)
)
delay <- filter(delay, count > 20, dest != "HNL")

# It looks like delays increase with distance up to ~750 miles 
# and then decrease. Maybe as flights get longer there's more 
# ability to make up delays in the air?
ggplot(data = delay, mapping = aes(x = dist, y = delay)) +
  geom_point(aes(size = count), alpha = 1/3) +
  geom_smooth(se = FALSE)
```

There are three steps to prepare this data:

1.  Group flights by destination.

1.  Summarize to compute distance, average delay, and number of flights.

1.  Filter to remove noisy points and Honolulu airport, which is almost
    twice as far away as the next closest airport.

This code is a little frustrating to write because we have to give each intermediate data frame a name, even though we don't care about it. Naming things is hard, so this slows down our analysis. 

There's another way to tackle the same problem with the pipe, `%>%`:

```{r}
delays <- flights %>% 
  group_by(dest) %>% 
  summarise(
    count = n(),
    dist = mean(distance, na.rm = TRUE),
    delay = mean(arr_delay, na.rm = TRUE)
  ) %>% 
  filter(count > 20, dest != "HNL")
```

This focuses on the transformations, not what's being transformed, which makes the code easier to read. You can read it as a series of imperative statements: group, then summarize, then filter. As suggested by this reading, a good way to pronounce `%>%` when reading code is "then".

Behind the scenes, `x %>% f(y)` turns into `f(x, y)`, and `x %>% f(y) %>% g(z)` turns into `g(f(x, y), z)` and so on. You can use the pipe to rewrite multiple operations in a way that you can read left-to-right, top-to-bottom. We'll use piping frequently from now on because it considerably improves the readability of code, and we'll come back to it in more detail in [pipes].

Working with the pipe is one of the key criteria for belonging to the tidyverse. The only exception is ggplot2: it was written before the pipe was discovered. Unfortunately, the next iteration of ggplot2, ggvis, which does use the pipe, isn't quite ready for prime time yet. 

### Missing values

You may have wondered about the `na.rm` argument we used above. What happens if we don't set it?

```{r}
flights %>% 
  group_by(year, month, day) %>% 
  summarise(mean = mean(dep_delay))
```

We get a lot of missing values! That's because aggregation functions obey the usual rule of missing values: if there's any missing value in the input, the output will be a missing value. Fortunately, _all aggregation functions have an `na.rm` argument_ which removes the missing values prior to computation:

```{r}
flights %>% 
  group_by(year, month, day) %>% 
  summarise(mean = mean(dep_delay, na.rm = TRUE))
```

In this case, where missing values represent cancelled flights, we could also tackle the problem by first removing the cancelled flights. We'll save this dataset so we can reuse in the next few examples.

##### not_cancelled ***

```{r}
not_cancelled <- flights %>% 
  filter(!is.na(dep_delay), !is.na(arr_delay))

```

```{r}
not_cancelled %>% 
  group_by(year, month, day) %>% 
  summarise(mean = mean(dep_delay))
```

### Counts

##### n(), sum(!is.na(x))

Whenever you do any aggregation, it's always a good idea to include either a count (`n()`), or a count of non-missing values (`sum(!is.na(x))`). That way you can check that you're not drawing conclusions based on very small amounts of data. For example, let's look at the planes (identified by their tail number) that have the highest average delays:

```{r}
delays <- not_cancelled %>% 
  group_by(tailnum) %>% 
  summarise(
    delay = mean(arr_delay)
  )

ggplot(data = delays, mapping = aes(x = delay)) + 
  geom_freqpoly(binwidth = 10)
```

Wow, there are some planes that have an _average_ delay of 5 hours (300 minutes)!

The story is actually a little more nuanced. We can get more insight if we draw a scatterplot of number of flights vs. average delay:

```{r}
delays <- not_cancelled %>% 
  group_by(tailnum) %>% 
  summarise(
    delay = mean(arr_delay, na.rm = TRUE),
    n = n()
  )

ggplot(data = delays, mapping = aes(x = n, y = delay)) + 
  geom_point(alpha = 1/10)
```

Not surprisingly, there is much greater variation in the average delay when there are few flights. The shape of this plot is very characteristic: whenever you plot a mean (or other summary) vs. group size, you'll see that the variation decreases as the sample size increases.

When looking at this sort of plot, it's often useful to filter out the groups with the smallest numbers of observations, so you can see more of the pattern and less of the extreme variation in the smallest groups. This is what the following code does, as well as showing you a handy pattern for integrating ggplot2 into dplyr flows. It's a bit painful that you have to switch from `%>%` to `+`, but once you get the hang of it, it's quite convenient.

```{r}
delays %>% 
  filter(n > 25) %>% 
  ggplot(mapping = aes(x = n, y = delay)) + 
    geom_point(alpha = 1/10)
```

--------------------------------------------------------------------------------

RStudio tip: a useful keyboard shortcut is Cmd/Ctrl + Shift + P. This resends the previously sent chunk from the editor to the console. This is very convenient when you're (e.g.) exploring the value of `n` in the example above. You send the whole block once with Cmd/Ctrl + Enter, then you modify the value of `n` and press Cmd/Ctrl + Shift + P to resend the complete block.

--------------------------------------------------------------------------------

There's another common variation of this type of pattern. Let's look at how the average performance of batters in baseball is related to the number of times they're at bat. Here I use data from the __Lahman__ package to compute the batting average (number of hits / number of attempts) of every major league baseball player.  

When I plot the skill of the batter (measured by the batting average, `ba`) against the number of opportunities to hit the ball (measured by at bat, `ab`), you see two patterns:

1.  As above, the variation in our aggregate decreases as we get more 
    data points.
    
2.  There's a positive correlation between skill (`ba`) and opportunities to 
    hit the ball (`ab`). This is because teams control who gets to play, 
    and obviously they'll pick their best players.

```{r}
# Convert to a tibble so it prints nicely
batting <- as_tibble(Lahman::Batting)

batters <- batting %>% 
  group_by(playerID) %>% 
  summarise(
    ba = sum(H, na.rm = TRUE) / sum(AB, na.rm = TRUE),
    ab = sum(AB, na.rm = TRUE)
  )

batters %>% 
  filter(ab > 100) %>% 
  ggplot(mapping = aes(x = ab, y = ba)) +
    geom_point() + 
    geom_smooth(se = FALSE)
```

This also has important implications for ranking. If you naively sort on `desc(ba)`, the people with the best batting averages are clearly lucky, not skilled:

```{r}
batters %>% 
  arrange(desc(ba))
```

You can find a good explanation of this problem at <http://varianceexplained.org/r/empirical_bayes_baseball/> and <http://www.evanmiller.org/how-not-to-sort-by-average-rating.html>.

### Useful summary functions {#summarise-funs}

Just using means, counts, and sum can get you a long way, but R provides many other useful summary functions:

##### mean(), median()

*   Measures of location: we've used `mean(x)`, but `median(x)` is also
    useful. The mean is the sum divided by the length; the median is a value 
    where 50% of `x` is above it, and 50% is below it.
    
    It's sometimes useful to combine aggregation with logical subsetting. 
    We haven't talked about this sort of subsetting yet, but you'll learn more
    about it in [subsetting].
    
    ```{r}
    not_cancelled %>% 
      group_by(year, month, day) %>% 
      summarise(
        avg_delay1 = mean(arr_delay),
        avg_delay2 = mean(arr_delay[arr_delay > 0]) # the average positive delay
      )
    ```

##### sd(), IQR(), mad()

*   Measures of spread: `sd(x)`, `IQR(x)`, `mad(x)`. The mean squared deviation,
    or standard deviation or sd for short, is the standard measure of spread.
    The interquartile range `IQR()` and median absolute deviation `mad(x)`
    are robust equivalents that may be more useful if you have outliers.
    
    ```{r}
    # Why is distance to some destinations more variable than to others?
    not_cancelled %>% 
      group_by(dest) %>% 
      summarise(distance_sd = sd(distance)) %>% 
      arrange(desc(distance_sd))
    ```
  
##### min(), quantile(), max()

*   Measures of rank: `min(x)`, `quantile(x, 0.25)`, `max(x)`. Quantiles
    are a generalisation of the median. For example, `quantile(x, 0.25)`
    will find a value of `x` that is greater than 25% of the values,
    and less than the remaining 75%.

    ```{r}
    # When do the first and last flights leave each day?
    not_cancelled %>% 
      group_by(year, month, day) %>% 
      summarise(
        first = min(dep_time),
        last = max(dep_time)
      )
    ```
  
##### first(), nth(), last()

*   Measures of position: `first(x)`, `nth(x, 2)`, `last(x)`. These work 
    similarly to `x[1]`, `x[2]`, and `x[length(x)]` but let you set a default 
    value if that position does not exist (i.e. you're trying to get the 3rd
    element from a group that only has two elements). For example, we can
    find the first and last departure for each day:
    
    ```{r}
    not_cancelled %>% 
      group_by(year, month, day) %>% 
      summarise(
        first_dep = first(dep_time), 
        last_dep = last(dep_time)
      )
    ```
    
    These functions are complementary to filtering on ranks. Filtering gives
    you all variables, with each observation in a separate row:
    
    ```{r}
    not_cancelled %>% 
      group_by(year, month, day) %>% 
      mutate(r = min_rank(desc(dep_time))) %>% 
      filter(r %in% range(r))
    ```

##### n(), sum(!is.na()), n_distinct(), count()

*   Counts: You've seen `n()`, which takes no arguments, and returns the 
    size of the current group. To count the number of non-missing values, use
    `sum(!is.na(x))`. To count the number of distinct (unique) values, use
    `n_distinct(x)`.
    
    ```{r}
    # Which destinations have the most carriers?
    not_cancelled %>% 
      group_by(dest) %>% 
      summarise(carriers = n_distinct(carrier)) %>% 
      arrange(desc(carriers))
    ```
    
    Counts are so useful that dplyr provides a simple helper if all you want is 
    a count:
    
    ```{r}
    not_cancelled %>% 
      count(dest)
    ```
    
    You can optionally provide a weight variable. For example, you could use 
    this to "count" (sum) the total number of miles a plane flew:
    
    ```{r}
    not_cancelled %>% 
      count(tailnum, wt = distance)
    ```
    
*   Counts and proportions of logical values: `sum(x > 10)`, `mean(y == 0)`.
    When used with numeric functions, `TRUE` is converted to 1 and `FALSE` to 0. 
    This makes `sum()` and `mean()` very useful: `sum(x)` gives the number of 
    `TRUE`s in `x`, and `mean(x)` gives the proportion.
    
    ```{r}
    # How many flights left before 5am? (these usually indicate delayed
    # flights from the previous day)
    not_cancelled %>% 
      group_by(year, month, day) %>% 
      summarise(n_early = sum(dep_time < 500))
    
    # What proportion of flights are delayed by more than an hour?
    not_cancelled %>% 
      group_by(year, month, day) %>% 
      summarise(hour_perc = mean(arr_delay > 60))
    ```

### Grouping by multiple variables

When you group by multiple variables, each summary peels off one level of the grouping. That makes it easy to progressively roll up a dataset:

##### daily ***
```{r}
daily <- group_by(flights, year, month, day)
(per_day   <- summarise(daily, flights = n()))
(per_month <- summarise(per_day, flights = sum(flights)))
(per_year  <- summarise(per_month, flights = sum(flights)))
```

__Be careful__ when progressively rolling up summaries: it's OK for sums and counts, but you need to think about weighting means and variances, and it's not possible to do it exactly for rank-based statistics like the median. In other words, the sum of groupwise sums is the overall sum, but the median of groupwise medians is not the overall median.

### Ungrouping

If you need to remove grouping, and return to operations on ungrouped data, use `ungroup()`. 

```{r}
daily %>% 
  ungroup() %>%             # no longer grouped by date
  summarise(flights = n())  # all flights
```

### Exercises

1.  Brainstorm at least 5 different ways to assess the typical delay 
    characteristics of a group of flights. Consider the following scenarios:
    
    * A flight is 15 minutes early 50% of the time, and 15 minutes late 50% of 
      the time.
      
    ```{r}
    # First, let's start with a simple sanity check. Do the results look logical when we look at a summary of number of flights for a given flight(number) and carrier comination as compared to how many of them respectivelty are 15 minutes early or late. 
    # Yes, the results look logical. 
      
    not_cancelled %>%
      group_by(flight, carrier) %>%
      summarize(
        n = n(),
        n_early_15 = sum(arr_delay <= -15),
        n_late_15  = sum(arr_delay >= 15)
        )
    ```

    ```{r}
    # That works, so now let's look at the results in terms of proportions:
    
    # Limit to at least 20 flights to remove small samples.
    # Group by carrier, flight and destination to eliminate overlapping designations.
    # Flights which arrived more than 15 minutes or more early 50% of the time, OR 15 minutes or more late 50% of the time.
    # (No flights averaged both.)
    
    not_cancelled %>%
      group_by(carrier, flight, dest)%>%
      summarize(
        n = n(),
        pct_early_15 = mean(arr_delay <= -15),
        pct_late_15  = mean(arr_delay >= 15)
      ) %>%
      filter(n >= 20, pct_early_15 >= .5 | pct_late_15 >= .5)
    
    ```
    
    * A flight is always 10 minutes late.
    
    ```{r}
    not_cancelled %>%
      group_by(carrier, flight, dest)%>%
      summarize(
        n = n(),
        pct_late_10 = mean(arr_delay >= 10)
      ) %>%
      filter(n >= 5, pct_late_10 == 1)
    ```
    

    * A flight is 30 minutes early 50% of the time, and 30 minutes late 50% of 
      the time.
      
    ```{r}
    not_cancelled %>%
      group_by(carrier, flight, dest)%>%
      summarize(
        n = n(),
        pct_early_30 = mean(arr_delay <= -30),
        pct_late_30 = mean(arr_delay >= 30)
      ) %>%
      filter(n >= 5, pct_early_30 >= .3, pct_late_30 >= .3)
    ```
      
      
    * 99% of the time a flight is on time. 1% of the time it's 2 hours late.
    
    ```{r}
    not_cancelled %>%
      group_by(carrier, flight, dest) %>%
      summarize(
        n = n(),
        on_time   = mean(arr_delay <= 0),
        late_2hrs = mean(arr_delay >= 120)
      ) %>%
      filter(n >20, on_time >= .99 | late_2hrs >= .01)
    ```
    
    ```{r}
    # Explore/check:
    flight_2901_9E <- not_cancelled %>%
      filter(carrier =='9E', flight == 2901) %>%
      arrange(desc(arr_delay))
    
    # Yes, the top three rows show that 3 out of 55 flights were more than 2 hours late = 5.45%

    flight_2901_9E
    
    ```
    
    Which is more important: arrival delay or departure delay?
    
    More context would be helpful to answer this question. Generally departure delay may be considered the most important since it is a proxy for cancelled flights. 


1.  Come up with another approach that will give you the same output as 
    `not_cancelled %>% count(dest)` and 
    `not_cancelled %>% count(tailnum, wt = distance)` (without using 
    `count()`).
    
    ```{r}
    #original
    not_cancelled %>% 
      count(dest)
    ```
    
    ```{r}
    #another appraoch
    not_cancelled %>%
      group_by(dest) %>%
      summarize(n = n())
    #original
    not_cancelled %>% 
      count(tailnum, wt = distance)
    #another approach
    not_cancelled %>%
      group_by(tailnum) %>%
      summarize(n = sum(distance))
    ```
    

1.  Our definition of cancelled flights (`is.na(dep_delay) | is.na(arr_delay)`
    ) is slightly suboptimal. Why? Which is the most important column?
    
    NA's for dep_delay and dep_time are both the same (8255), whereas arr_time (8713) and arr_delay (9430) likely include missing data that was simply not recorded, or even flights that departed and failed to arrive dur to crashes or other extenuating circumstances. Alternately, for analysis we might want to exlude the arrival na's in cases where they become contagious. 

    ```{r}
    summary(flights)
    
    ```

    Out of curiousity: An alternate way to count na's by column: https://sebastiansauer.github.io/sum-isna/

    ```{r}
    flights %>%
      select(everything()) %>%
      summarize_all(funs(sum(is.na(.))))
    
    ```


1.  Look at the number of cancelled flights per day. Is there a pattern?
    Is the proportion of cancelled flights related to the average delay?
    
    Yes, there is a pattern. On a typical day in 2013, the number of flights cancelled was less than .5% and the average dpearture delay was less than 15 minutes. 
 
    However, on both typical days and atypical days, as the percentage of cancellations increases, so does the average delay. This trend holds up to 25% cancellations, whereafter the number of observations becomes exceedingly smaller. 
 
    After 25% cancellations on a given day, one might assume only flights which are not affected extraordinary circumstances are permitted to fly, and therefore are not impacted by excessive delay. 

    ```{r}
    flights %>%
      group_by(year, month, day) %>%
      summarize(num_flights = n(),
                num_cancelled = sum(is.na(dep_delay)),
                pct_cancelled = mean(is.na(dep_delay)),
                avg_delay     = mean(dep_delay, na.rm = TRUE)
                ) %>%
      ggplot(mapping = aes(x = pct_cancelled * 100, y = avg_delay)) +
      geom_point(aes(alpha = 1/20))+
      geom_smooth(se = FALSE)
    ```

    So, out of curiousity, which days had an inordinate number of cancellations, more than 50%?

    ```{r}   
    flights %>%
      group_by(year, month, day) %>%
      summarize(num_flights = n(),
                num_cancelled = sum(is.na(dep_delay)),
                pct_cancelled = mean(is.na(dep_delay)),
                avg_delay     = mean(dep_delay, na.rm = TRUE)
                ) %>%
      filter(pct_cancelled > .5)
    ```

    February 8th and 9th, 2013. What was in the news? Blizzard in the northeast US https://www.theguardian.com/world/2013/feb/08/4000-flights-cancelled-snow-us


1.  Which carrier has the worst delays? Challenge: can you disentangle the
    effects of bad airports vs. bad carriers? Why/why not? (Hint: think about
    `flights %>% group_by(carrier, dest) %>% summarise(n())`)
    
    ```{r}
    not_cancelled %>%
      group_by(carrier) %>%
      summarize(
        avg_dep_delay = mean(dep_delay),
        avg_arr_delay = mean(arr_delay)
      ) %>%
      arrange(desc(avg_arr_delay))
    
    # Carrier F9 has the worst average arrival delay, being 21.92 minutes
    ```
    
    ```{r}

    # plot
    plot.data <- not_cancelled %>%
    group_by(carrier) %>%
    summarize(
    avg_dep_delay = mean(dep_delay),
    avg_arr_delay = mean(arr_delay)
    )
    ggplot(plot.data) +
    geom_col(mapping = aes(x = carrier, y = avg_arr_delay))
    
    ```
    
    ```{r}

    # effects of bad airports vs. bad carriers
    # we already know carrier F9 has the worst average arrival delay

    # which combination of carrier and destination has the worst average arrival delay?

    not_cancelled %>%
      group_by(carrier, dest) %>%
      summarize(
        num_flights = n(),
        avg_arr_delay_per_dest = mean(arr_delay)
      ) %>%
      arrange(desc(avg_arr_delay_per_dest))
    
    # UA into STL, but only 2 flights. 
    ```
    
    ```{r}
    
    # let's look at sample of 20 or more flights
    
    not_cancelled %>%
      group_by(carrier, dest) %>%
      summarize(
        num_flights = n(),
        avg_arr_delay_per_dest = mean(arr_delay)
      ) %>%
      arrange(desc(avg_arr_delay_per_dest)) %>%
      filter(num_flights >= 20)
    
    # EV into CAE tops the list. In fact carrier EV holds the 6 of the top 7 spots.
    # F9 shows up at number 9 into DEN.
    ```
    
    ```{r}

    # which airport has the worst average arrival delay?
    not_cancelled %>%
      group_by(dest) %>%
      summarize(
        num_flights = n(),
        avg_arr_delay_at_dest = mean(arr_delay)
      ) %>%
      arrange(desc(avg_arr_delay_at_dest))
    # Indeed CAE has the worst average arrival delay of any destination
    ```
    
    ```{r}

    # So what might be causing F9 to top the list of carriers with the worst average arrival delay?
    
    not_cancelled %>%
      group_by(carrier, dest) %>%
      summarize(
        num_flights = n(),
        avg_arr_delay_per_dest = mean(arr_delay)
      ) %>%
      arrange(desc(avg_arr_delay_per_dest)) %>%
      filter(carrier == 'F9')
    
    # Carrier F9 only flies into DEN
    ```
    
    ```{r}
    
    # What about carrier EV?
    
    not_cancelled %>%
      group_by(carrier, dest) %>%
      summarize(
        num_flights = n(),
        avg_arr_delay_per_dest = mean(arr_delay)
      ) %>%
      arrange(desc(avg_arr_delay_per_dest)) %>%
      filter(carrier == 'EV')
    
    # EV flies into many destinations where their avg arrival delay is relative very high, however it also flies into 51 more destinations. 
    ```
    
    ```{r}
    # As yet, we have not compared the delays weighted against number of flights, but let's plot EV's delays per destination.

    not_cancelled %>% 
      group_by(carrier, dest) %>%
      summarize(
        num_flights = n(),
        avg_arr_delay_per_dest = mean(arr_delay)
      ) %>%
      filter(carrier == 'EV') %>%
      ggplot(mapping = aes(x = dest, y = avg_arr_delay_per_dest)) +
      geom_col() +
      coord_flip()
    
    # Plot shows most of carrier EV's average arrival delays into its 61 destinations are 20 minutes or less.
    
    # While it has a relatively high average arrival delay as a carrier, and it also holds many top spots 
    
    # For carrier-destination combo's, EV's overall average arrival delay is tempered by the fact it has 61 diverse destinations.

    ```

1.  What does the `sort` argument to `count()` do. When might you use it?

    Sort = TRUE will sort output in descending order of n

    ```{r}
    not_cancelled %>%
      count(dest, sort = TRUE)
    ```
    Which is easier than:

    ```{r}    
    not_cancelled %>%
      group_by(dest) %>%
      summarize(n =n()) %>%
      arrange(desc(n))
    ```

    Out of curiosity, what were the maximum arrival delays at each respective destination ... ouch: 
    
    ```{r}
    not_cancelled %>%
      group_by(dest)%>%
      summarize(
        n = n(),
        max_arr_delay = max(arr_delay)
        )%>%
      arrange(desc(max_arr_delay))
    ```


## Grouped mutates (and filters)

Grouping is most useful in conjunction with `summarise()`, but you can also do convenient operations with `mutate()` and `filter()`:

*   Find the worst members of each group:

    ```{r}
    flights_sml %>% 
      group_by(year, month, day) %>%
      filter(rank(desc(arr_delay)) < 10)
    ```

*   Find all groups bigger than a threshold:

    ```{r}
    popular_dests <- flights %>% 
      group_by(dest) %>% 
      filter(n() > 365)
    popular_dests
    ```

*   Standardize to compute per group metrics:

    ```{r}
    popular_dests %>% 
      filter(arr_delay > 0) %>% 
      mutate(prop_delay = arr_delay / sum(arr_delay)) %>% 
      select(year:day, dest, arr_delay, prop_delay)
    ```

A grouped filter is a grouped mutate followed by an ungrouped filter. I generally avoid them except for quick and dirty manipulations: otherwise it's hard to check that you've done the manipulation correctly.

Functions that work most naturally in grouped mutates and filters are known as  window functions (vs. the summary functions used for summaries). You can learn more about useful window functions in the corresponding vignette: `vignette("window-functions")`.

### Exercises

1.  Refer back to the lists of useful mutate and filtering functions. 
    Describe how each operation changes when you combine it with grouping.
    
    [Useful creation functions](#mutate-funs)

    * +, - etc
    * log()
    * lead(), lag()
    * dense_rank(), min_rank(), percent_rank(), row_number(), cume_dist(), ntile()
    * cumsum(), cummean(), cummin(), cummax(), cumany(), cumall()
    * na_if(), coalesce()
    * if_else(), recode(), case_when()
    

1.  Which plane (`tailnum`) has the worst on-time record?

    Tailnum __N988AT__ has the worst on-time record in terms of percentage of late arrivals _for more than twenty flights_. 
    
    ```{r}
    not_cancelled %>% 
      group_by(tailnum) %>% 
      summarise(
        n = n(),
        pct_arr_delay = sum(arr_delay > 0) / n()
      ) %>% 
      filter(n > 20) %>% 
      filter(min_rank(desc(pct_arr_delay)) <= 3) %>% 
      arrange(desc(pct_arr_delay))
    ```
    
    Alternately, using top_n(1, x), which is the same as if using above filter(min_rank(desc(x)) <= 1)
    
    ```{r}
    not_cancelled %>% 
      group_by(tailnum) %>%
      summarise(
        n= n(),
        late = sum(arr_delay > 0),
        pct_arr_delay =  late / n
        ) %>% 
      filter(n > 20) %>% 
      top_n(1, pct_arr_delay)
    ```
   
   
1.  What time of day should you fly if you want to avoid delays as much
    as possible?
    
    On average, avoid __7:00PM to 10:00 PM__ if you want to avoid delays as much
    as possible.
    
    ```{r}
    not_cancelled %>% 
      group_by(hour) %>% 
      summarise(
        avg_dep_delay = mean(dep_delay)
      ) %>% 
      filter(min_rank(desc(avg_dep_delay)) <= 4) %>% 
      arrange(desc(avg_dep_delay))
    ```
    
1.  For each destination, compute the total minutes of delay. For each, 
    flight compute the proportion of the total delay for its destination.
    
    ```{r}
    # For each destination, compute the total minutes of delay.
    
    not_cancelled %>% 
      filter(arr_delay > 0) %>% 
      group_by(dest, flight) %>% 
      summarize(
        total_mins = sum(arr_delay)
        #prop_delay = arr_delay / total_mins
      ) %>% 
      select(dest, flight, total_mins) %>% 
      arrange(dest)
    ``` 
    
    ```{r}
    by_dest <- not_cancelled %>% 
      filter(arr_delay > 0) %>%   # filter by rows wih arival delays > 0
      group_by(dest) %>%
      mutate(
        total_dest_delay = sum(arr_delay)
        )
    
    # The total_dest_delay gets mutated to each row, respective to the row's dest, and thus is availble to be used below as the detominator in order to calculate the proportional delay.
     
    
    by_flight <- by_dest %>% 
      group_by(flight) %>% 
      mutate(
        prop_delay = sum(arr_delay) / total_dest_delay
        )
    
    # Ungrouping is necessary in order to regroup and summarize. else the results would be a rather uninterptretable as 133K largely non-unique rows, rather than 8K unique rows.  
    
    by_flight %>% 
      ungroup() %>% 
      group_by(dest, flight, total_dest_delay, prop_delay) %>% 
      summarize(
      )
    
    ```    
    
1.  Delays are typically temporally correlated: even once the problem that
    caused the initial delay has been resolved, later flights are delayed 
    to allow earlier flights to leave. Using `lag()` explore how the delay
    of a flight is related to the delay of the immediately preceding flight.
    
    Airports can be big. It may be helpful to have gate information. Lacking gate information, let's try using as a proxy one carrier at one origin since carriers tend to be assigned one or more specific gates. 
    
    ```{r warning=FALSE}
    
    not_cancelled %>%
      filter(carrier == "DL", origin == 'LGA', month == 1, dep_delay > 0) %>% 
      arrange(month, day, sched_dep_time) %>% 
      mutate(
        prior_delay = lag(dep_delay)
      ) %>% 
      ggplot() +
      geom_smooth(aes(dep_delay, prior_delay), se = FALSE)
    
    ```
    
1.  Look at each destination. Can you find flights that are suspiciously
    fast? (i.e. flights that represent a potential data entry error). Compute
    the air time a flight relative to the shortest flight to that destination.
    Which flights were most delayed in the air?
    
    Using the he 68–95–99.7 rule as a naive guidline, there are 19 out of 327,000 non-cancelled flights for which the recorded shortest air_time is outside the band of three standard deviations. 
    
    ```{r}
    not_cancelled %>%
      group_by(origin, dest) %>% 
      mutate(
        average = mean(air_time, na.rm = TRUE),
        std_dev = sd(air_time),
        x3std_dev = 3 * std_dev,
        avg_less_x3_sd = average - x3std_dev
      ) %>% 
      filter(min_rank(air_time) <= 1, air_time < avg_less_x3_sd) %>% 
      select(origin, dest, average, std_dev, x3std_dev, avg_less_x3_sd, shortest = air_time) %>% 
      arrange(shortest)
    ```
    
1.  Find all destinations that are flown by at least two carriers. Use that
    information to rank the carriers.

1.  For each plane, count the number of flights before the first delay 
    of greater than 1 hour.